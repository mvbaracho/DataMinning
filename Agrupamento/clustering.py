# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e_3P1TwoLmB4DjREyWx9y63onusJlnd4

## **SETUP**
"""

# Commented out IPython magic to ensure Python compatibility.
#BIBLIOTECAS
#Imports
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
import pandas as pd
import seaborn as sns
import statistics
import datetime as dt
import calendar
import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import euclidean_distances
from sklearn import preprocessing
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.decomposition import PCA

# %matplotlib inline

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':"138bRT5WPXfCdZj2fS_sMars4NlaQr75y"})
downloaded.GetContentFile('Sales2.csv')

# CARREGANDO DB
df = pd.read_csv('Sales2.csv', sep=';', na_values='?')

df.head()

"""## **Pré-Processamento**"""

#PRÉ-PROCESSAMENTO DA BASE DE DADOS

#sns.heatmap(df.isna())
df['Timestamp']= pd.to_datetime(df['Timestamp'],format='%Y-%m-%d')
df['year']= df['Timestamp'].dt.year
df['month']= df['Timestamp'].dt.month
df['day']= df['Timestamp'].dt.day
df['hour'] = df['Timestamp'].dt.hour
df['Weekday'] = df['Timestamp'].dt.day_name()
#df['PaymentMethod'] = np.where(df['PaymentMethod'] > 1, 'Credit Card', 'Cash')
#df['PeriodDay'] = np.where((df['hour'] > 0) & (df['hour']<=6), 0, 1)
b = [0,6,12,18,24]
l = [0, 1, 2,3]
df['session'] = pd.cut(df['hour'], bins=b, labels=l, include_lowest=True)
df = df.drop(columns=['Timestamp', 'Retailer', 'Pdv'])
df

# Pré-processamento -> LabelEncoder para transformar strings para inteiros
le = preprocessing.LabelEncoder()
df['City'] = le.fit_transform(df['City'])
df['State'] = le.fit_transform(df['State'])
df['Weekday'] = le.fit_transform(df['Weekday'])
df['session'] = le.fit_transform(df['session'])

"""## **Técnica K-Means**"""

#Início K-Means
# Inicializa o K-Means com 3 centroides randômicos
kmeans = KMeans(n_clusters = 3, init = 'random')
# Faz o agrupamento dos dados semelhantes
kmeans.fit(df)
# Obtém os centroides que foram gerados
centroids = kmeans.cluster_centers_
centroids

# Pega cada registro e computa a distância pros centroides
distance = kmeans.fit_transform(df)
distance

# As labels possuem 3 classes diferentes sendo geradas corretamente
labels = kmeans.labels_
labels

# Método Elbow: verifica a quantidade ideal de clusters para o nosso problema
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = 'random')
    kmeans.fit(df)
    print(i,kmeans.inertia_)
    wcss.append(kmeans.inertia_)  
plt.plot(range(1,11), wcss)
plt.title('O Metodo Elbow')
plt.xlabel('Numero de Clusters')
plt.ylabel('WSS') #within cluster sum of squares
plt.show()

plt.scatter(df.iloc[:,0], df.iloc[:,9]) #posicionamento dos eixos x e y
plt.scatter(kmeans.cluster_centers_[:,1],kmeans.cluster_centers_[:,0], s = 70, c = 'red') #posição de cada centroide no gráfico
plt.show()
# Usar gráfico de barras eixo x -> label (0,1,2,3)  X -> Qtd. de elementos p/grupo

df['Clusters'] = labels  #Criando a coluna relativa aos clusters

df[df['Clusters'] == 8]

# Plotando o gráfico: 3 cores representando 3 clusters diferentes
fig = plt.figure(figsize= (6,6))

ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Preço', fontsize = 15)
ax.set_ylabel('Período do dia', fontsize = 15)
ax.set_title('Componentes principais', fontsize = 20)

colors = np.array(["blue", "yellow", "green"])
ax.scatter(x=df.Price, y=df.session, c=colors[df.Clusters], s=50)
plt.show()

df['Clusters'].value_counts()

plt.bar(df['Clusters'].value_counts().keys(), df['Clusters'].value_counts().values, color=['red', 'green', 'black'])
plt.xlabel('Grupos')
plt.ylabel('Quantidade')
plt.xticks([0,1,2])
plt.show()

"""## **K-means e PCA**

> Redução da dimensionalidade linear usando a decomposição de valores singulares dos dados para projetá-los em um espaço dimensional inferior.


"""

# Dropando a coluna clusters do K-means anterior
df = df.drop(columns='Clusters')
df

# Normalização dos dados -> Eliminação de discrepâncias

new_df = ((df - df.min()) / (df.max() - df.min()))
new_df

#Método Elbow
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, max_iter=300)
    kmeans.fit(new_df)
    print(i,kmeans.inertia_)
    wcss.append(kmeans.inertia_)  
plt.plot(range(1, 11), wcss)
plt.title('O Metodo Elbow')
plt.xlabel('Numero de Clusters')
plt.ylabel('WSS') #within cluster sum of squares
plt.show()

clustering = KMeans(n_clusters=3, max_iter=300)
clustering.fit(new_df) #Aplica o KMeans para a base

new_df['Clusters'] = clustering.labels_ #Cria uma nova coluna de acordo com o cluster de cada registro
new_df

#Técnica de PCA para obter as principais variáveis
#para entender melhor: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_newdf = pca.fit_transform(new_df)
pca_newdf
pca_final = pd.DataFrame(data = pca_newdf, columns= ['Componente_1', 'Componente_2']) #Seta as duas principais variáveis a agrupar
pca_final

pca_df_final = pd.concat([pca_final, new_df[['Clusters']]], axis=1) # Concatena o pca_final com a coluna de Clusters do new_df
pca_df_final

fig = plt.figure(figsize= (6,6))

ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Componente 1', fontsize = 15)
ax.set_ylabel('Componente 2', fontsize = 15)
ax.set_title('Componentes principais', fontsize = 20)

targets = [0, 1, 2]
colors = ['r', 'g', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = pca_df_final['Clusters'] == target
    ax.scatter(pca_df_final.loc[indicesToKeep, 'Componente_1']
               , pca_df_final.loc[indicesToKeep, 'Componente_2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
plt.show()

"""## **K-Means utilizando apenas a relação Preço x Recorrência:  Agrupando em faixas de valores**"""

novo = pd.DataFrame({'Price': df['Price'].value_counts().keys(), 'Qtd': df['Price'].value_counts().values})
novo

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'random')
    kmeans.fit(novo)
    print(i,kmeans.inertia_)
    wcss.append(kmeans.inertia_)  
plt.plot(range(1, 11), wcss)
plt.title('O Metodo Elbow')
plt.xlabel('Numero de Clusters')
plt.ylabel('WSS') #within cluster sum of squares
plt.show()

clustering = KMeans(n_clusters=3, max_iter=300)
clustering.fit(novo) #Aplica o KMeans para a base

novo['Cluster'] = clustering.labels_
novo

centroides = clustering.cluster_centers_
centroides

plt.scatter(novo.loc[novo['Cluster'] == 0, ['Price']], novo.loc[novo['Cluster'] == 0, ['Qtd']], s=50, c='orange', label='Faixa1')
plt.scatter(novo.loc[novo['Cluster'] == 1, ['Price']], novo.loc[novo['Cluster'] == 1, ['Qtd']], s=50, c='blue', label='Faixa1')
plt.scatter(novo.loc[novo['Cluster'] == 2, ['Price']], novo.loc[novo['Cluster'] == 2, ['Qtd']], s=50, c='red', label='Faixa1')
plt.scatter(centroides[:,0],centroides[:,1], s = 200, c = 'black', label='Centroides') #posição de cada centroide no gráfico
plt.title('K-means relação preço-recorrência')
plt.xlabel('Preço')
plt.ylabel('Quantidade')
plt.legend()
plt.show()

"""## **DBSCAN**"""

# EPS: é a distância máxima entre dois pontos pra que sejam considerados vizinhos |
# min_samples: quantidade de amostras em uma vizinhança pra um ponto ser considerado um ponto central 
db = DBSCAN(eps=1.2, min_samples=7)
db.fit(df[:10000])
db

labels = db.labels_

# Commented out IPython magic to ensure Python compatibility.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
# n_noise representa a quantidade de amostras ruidosas: amostras ruidosas são pontos que não se enquadram em nenhum cluster
print('Estimated number of noise points: %d' % n_noise_)
# Coeficiente que mede a distância média entre uma amostra e um cluster ao qual ela não pertence
# O índice de Silhouette varia entre -1 e 1
#Índice de Silhouette: Valores próximos a 0 indicam clusters sobrepostos
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(df[:10000], labels))

#Lista de labels
unique_labels = list(set(labels))

#lista de quantidade de label
l = []
labels = list(labels)
for i in unique_labels: 
  l.append(labels.count(i))

# GRÁFICO #
plt.bar(unique_labels, l)
plt.show()