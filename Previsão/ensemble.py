# -*- coding: utf-8 -*-
"""Previsão.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11IWsIAFGRY5kDf6vONwHLLa6yl0xq0-6

# SETUP
"""

# Commented out IPython magic to ensure Python compatibility.
#BIBLIOTECAS
#!pip install pycaret
#!pip install scikit-learn

import datetime as dt
import pandas as pd
import matplotlib.pyplot as plt
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing

import numpy as np  
import seaborn as seabornInstance 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.svm import SVR
from sklearn.metrics import r2_score,mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV


from numpy import std
from numpy import mean
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.neural_network import MLPRegressor

from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import StackingClassifier
from matplotlib import pyplot

# %matplotlib inline

# IMPORTAÇÃO DB
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':"138bRT5WPXfCdZj2fS_sMars4NlaQr75y"})
downloaded.GetContentFile('Sales2.csv')

# CARREGANDO DB
df = pd.read_csv('Sales2.csv', sep=';', na_values='?')

"""# Database SETUP"""

# Trata a coluna Timestamp
df['Timestamp']= pd.to_datetime(df['Timestamp'],format='%Y-%m-%d')

# Obtém a coluna Price(Somatório das vendas do dia)
df_class2 = df[['Timestamp', 'Price']].groupby(pd.Grouper(key='Timestamp', freq='1D')).sum()['Price']

# A partir daqui, um novo dataframe será gerado(df2) para trabalharmos
df2 = pd.DataFrame(df_class2)

df2['Price']

df2 = df2.head(n=len(df2))

fig, ax = pyplot.subplots()
pyplot.plot(range(len(df2['Price'])), df2['Price'])
fig.set_size_inches(30, 10.5, forward=True)
pyplot.show()

df2

# Normalização dos dados
scaler = MinMaxScaler()
df_n = scaler.fit_transform(df2)
#df_n

#from sklearn.linear_model import LogisticRegression
from sklearn import metrics


# Conjunto de treinamento e teste
#X = df['Timestamp'].values.reshape(-1,1)
X = df_n[:,0]
print(X.shape)
y = [0,0]
for i in range(len(X)-2):
  y.append(X[i+1])
y = np.array(y)
print(y.shape)
#y = df['Price'].values.reshape(-1,1)
y

df_n = pd.DataFrame({'X': X, 'y':y})
df_n

fig, ax = pyplot.subplots()
pyplot.plot(range(len(X)), X)
pyplot.plot(range(len(y)), y)
fig.set_size_inches(30, 10.5, forward=True)
pyplot.show()

"""# Previsão"""

df_results_rmse = pd.DataFrame()

## GRID SEARCH : Linear Regression

results_rmse_lr = []
lr = LinearRegression()  


X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {'n_jobs':[1, 2, 3, 4, 5, 10]}
lr = LinearRegression()
clf = GridSearchCV(lr, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Linear Regression

results_rmse_lr = []
lr = LinearRegression(clf.best_params_['n_jobs'])  


for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1),
                                                      df_n['y'].values.reshape(-1,1), 
                                                      test_size=0.2, 
                                                      shuffle=False)
  
  # Treinamento
  lr.fit(X_train, y_train)
  #print(X_train[1,0])

  # Teste
  y_pred_LR = lr.predict(X_test)
  # Metrics
  mse_LR = mean_squared_error(y_test, y_pred_LR)
  rmse_LR = np.sqrt(mse_LR)
  results_rmse_lr.append(rmse_LR)
df_results_rmse['lr'] = results_rmse_lr

##Imprimindo o dataFrame LR
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_LR_desnormalized = scaler.inverse_transform(y_pred_LR)
# Visualising the results
df_lr = pd.DataFrame()
df_lr['real'] = y_test_desnormalized[:,0].tolist()
df_lr['predicted lr'] = y_pred_LR_desnormalized[:,0].tolist()
df_lr

# Visualising the results LR
plt.plot(df_lr.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_lr.index, y_pred_LR_desnormalized, color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : Support Vector Regression (SVR)
results_rmse_svr = []
from sklearn import svm

X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {'C':[0.1, 1, 10, 100], 'epsilon':[0.1, 0.01, 0.0001], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf', 'linear', 'poly']}
svr = SVR()
clf = GridSearchCV(svr, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Support Vector Regression (SVR)
results_rmse_svr = []
from sklearn import svm

for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)#stratify=None, random_state=None)


# Treinamento
#for i in range(0, 30):
  supportVectorRegModel = svm.SVR(C = clf.best_params_['C'], epsilon = clf.best_params_['epsilon'], gamma = clf.best_params_['gamma'], kernel = clf.best_params_['kernel'])
  supportVectorRegModel.fit(X_train, y_train)
  # Teste
  y_pred_SVR = supportVectorRegModel.predict(X_test)

  # Métrics
  mse_SVR = mean_squared_error(y_test, y_pred_SVR)
  rmse_SVR = np.sqrt(mse_SVR)
  rmse_SVR
  results_rmse_svr.append(rmse_SVR)
df_results_rmse['svr'] = results_rmse_svr

##Imprimindo o dataFrame svr
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_svr_desnormalized = scaler.inverse_transform([y_pred_SVR])
# Visualising the results
df_svr = pd.DataFrame()
df_svr['real'] = y_test_desnormalized[:,0].tolist()
df_svr['predicted svr'] = y_pred_svr_desnormalized.tolist()[0]
df_svr

# Visualising the results SVR
plt.plot(df_svr.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_svr.index, y_pred_svr_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : RANDOM FOREST
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
   # 'max_features': [1, 2, 3],
    'min_samples_leaf': [1, 3, 4, 5],
    'min_samples_split': [0.1, 1.0]}
    #'n_estimators': [1, 100, 200, 300, 1000]}
rf = RandomForestRegressor()
clf = GridSearchCV(rf, param_grid=parameters)
clf.fit(X_train, y_train)

clf.best_params_

## PREDICTIONS : Random Forest
results_rmse_rf = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, train_size=0.1)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  rf = RandomForestRegressor(bootstrap = clf.best_params_['bootstrap'], max_depth = clf.best_params_['max_depth'], min_samples_leaf = clf.best_params_['min_samples_leaf'], min_samples_split = clf.best_params_['min_samples_split'])   
  rf.fit(X_train, y_train) 
  # Teste
  y_pred_RF = rf.predict(X_test)

  # Metrics
  mse_RF = mean_squared_error(y_test, y_pred_RF)
  rmse_RF = np.sqrt(mse_RF)
  results_rmse_rf.append(rmse_RF)
df_results_rmse['rf'] = results_rmse_rf

##Imprimindo o dataFrame Random Forest
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_rf_desnormalized = scaler.inverse_transform([y_pred_RF])
# Visualising the results
df_rf = pd.DataFrame()
df_rf['real'] = y_test_desnormalized[:,0].tolist()
df_rf['predicted rf'] = y_pred_rf_desnormalized.tolist()[0]
df_rf

# Visualising the results RF
plt.plot(df_rf.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_rf.index, y_pred_rf_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : KNeighbors
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {'n_neighbors': [1, 3, 5, 10, 14, 19],
              'weights': ['uniform', 'distance'],
              'metric': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'],
              'p':[1, 2],
              'leaf_size': [1, 20, 40]
              }
knn = KNeighborsRegressor()
clf = GridSearchCV(knn, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : KNeighbor
results_rmse_knn = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, train_size=0.1)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  knn = KNeighborsRegressor(metric = clf.best_params_['metric'], weights = clf.best_params_['weights'], p = clf.best_params_['p'], leaf_size = clf.best_params_['leaf_size']) 
  knn.fit(X_train, y_train) 
  # Teste
  y_pred_knn = knn.predict(X_test)

  # Metrics
  mse_knn = mean_squared_error(y_test, y_pred_knn)
  rmse_knn = np.sqrt(mse_knn)
  results_rmse_knn.append(rmse_knn)
df_results_rmse['knn'] = results_rmse_knn

##Imprimindo o dataFrame knn
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_knn_desnormalized = scaler.inverse_transform(y_pred_knn)
# Visualising the results
df_knn = pd.DataFrame()
df_knn['real'] = y_test_desnormalized[:,0].tolist()
df_knn['predicted knn'] = y_pred_knn_desnormalized[:,0].tolist()
df_knn

# Visualising the results SVR
plt.plot(df_knn.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_knn.index, y_pred_knn_desnormalized, color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : Gradient Boosting Regressor
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {
    'learning_rate': [0.001, 0.01, 0.1],
    'min_samples_split': [50, 100],
    'min_samples_leaf': [50, 100],
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [2, 4, 6, 8, 10]}

gbr = GradientBoostingRegressor()
clf = GridSearchCV(gbr, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Gradient Boosting Regressor
results_rmse_gbr = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  gbr = GradientBoostingRegressor(learning_rate = clf.best_params_['learning_rate'], max_depth = clf.best_params_['max_depth'], min_samples_leaf = clf.best_params_['min_samples_leaf'], n_estimators = clf.best_params_['n_estimators']) #n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls'
  gbr.fit(X_train, y_train) 
  # Teste
  y_pred_gbr = gbr.predict(X_test)
  # Metrics
  mse_gbr = mean_squared_error(y_test, y_pred_gbr)
  rmse_gbr = np.sqrt(mse_gbr)
  results_rmse_gbr.append(rmse_gbr)
df_results_rmse['gbr'] = results_rmse_gbr

##Imprimindo o dataFrame Gradient Boosting Regressor
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_gbr_desnormalized = scaler.inverse_transform([y_pred_gbr])
# Visualising the results
df_gbr = pd.DataFrame()
df_gbr['real'] = y_test_desnormalized[:,0].tolist()
df_gbr['predicted gbr'] = y_pred_gbr_desnormalized.tolist()[0]
df_gbr

# Visualising the results GBR
plt.plot(df_gbr.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_gbr.index, y_pred_gbr_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : AdaBoost Regressor
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {"n_estimators":[100,500],"learning_rate":list(np.linspace(0.01,1,10)),"loss":["linear", "square", "exponential"]}
abr = AdaBoostRegressor()
clf = GridSearchCV(abr, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : AdaBoost Regressor
results_rmse_abr = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  abr = AdaBoostRegressor(learning_rate = clf.best_params_['learning_rate'], loss= clf.best_params_['loss'], n_estimators=clf.best_params_['n_estimators']) 
  abr.fit(X_train, y_train) 
  # Teste
  y_pred_abr = abr.predict(X_test)

  # Metrics
  mse_abr = mean_squared_error(y_test, y_pred_gbr)
  rmse_abr = np.sqrt(mse_abr)
  results_rmse_abr.append(rmse_abr)
df_results_rmse['abr'] = results_rmse_abr

##Imprimindo o dataFrame abr
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_abr_desnormalized = scaler.inverse_transform([y_pred_abr])
# Visualising the results
df_abr = pd.DataFrame()
df_abr['real'] = y_test_desnormalized[:,0].tolist()
df_abr['predicted abr'] = y_pred_abr_desnormalized.tolist()[0]
df_abr

# Visualising the results ABR
plt.plot(df_abr.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_abr.index, y_pred_abr_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : Ridge
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = { 'alpha': list(x / 10 for x in range(0, 101)), "fit_intercept": [True, False], "solver": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}

rcv = Ridge() 
clf = GridSearchCV(rcv, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Ridge

# Treinamento
results_rmse_rcv = []

for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)
  rcv = Ridge(alpha = clf.best_params_['alpha'], fit_intercept=clf.best_params_['fit_intercept'], solver=clf.best_params_['solver']) 
  rcv.fit(X_train, y_train) 
  # Teste
  y_pred_rcv = rcv.predict(X_test)

  # Metrics
  mse_rcv = mean_squared_error(y_test, y_pred_rcv)
  rmse_rcv = np.sqrt(mse_rcv)
  results_rmse_rcv.append(rmse_rcv)
df_results_rmse['rcv'] = results_rmse_rcv

##Imprimindo o dataFrame Ridge
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_rcv_desnormalized = scaler.inverse_transform(y_pred_rcv)
# Visualising the results
df_rcv= pd.DataFrame()
df_rcv['real'] = y_test_desnormalized[:,0].tolist()
df_rcv['predicted svr'] = y_pred_rcv_desnormalized[:,0].tolist()
df_rcv

# Visualising the results rcv
plt.plot(df_rcv.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_rcv.index, y_pred_rcv_desnormalized, color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : Decision Tree
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {"criterion": ["mse", "mae"],
              "min_samples_split": [10, 20, 40],
              "max_depth": [2, 6, 8],
              "min_samples_leaf": [20, 40, 100],
              "max_leaf_nodes": [5, 20, 100],
              }

dt = DecisionTreeRegressor() 
clf = GridSearchCV(dt, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Decision Tree
results_rmse_dt = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  dt = DecisionTreeRegressor(criterion = clf.best_params_['criterion'], max_depth = clf.best_params_['max_depth'], max_leaf_nodes = clf.best_params_['max_leaf_nodes'], min_samples_leaf = clf.best_params_['min_samples_leaf'], min_samples_split = clf.best_params_['min_samples_split']) 
  dt.fit(X_train, y_train) 
  # Teste
  y_pred_dt = dt.predict(X_test)

  # Metrics
  mse_dt = mean_squared_error(y_test, y_pred_dt)
  rmse_dt = np.sqrt(mse_dt)
  results_rmse_dt.append(rmse_dt)
df_results_rmse['dt'] = results_rmse_dt

##Imprimindo o dataFrame dt
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_dt_desnormalized = scaler.inverse_transform([y_pred_dt])
# Visualising the results
df_dt = pd.DataFrame()
df_dt['real'] = y_test_desnormalized[:,0].tolist()
df_dt['predicted dt'] = y_pred_dt_desnormalized.tolist()[0]
df_dt

# Visualising the results dt
plt.plot(df_dt.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_dt.index, y_pred_dt_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## GRID SEARCH : Extra Tree
X_train, X_test, y_train, y_test = train_test_split(
  df_n['X'].values.reshape(-1,1), 
  df_n['y'].values.reshape(-1,1), 
  test_size=0.3, 
  shuffle=False)
  
#GridSearch (seleção de parâmetros)
parameters = {"criterion": ["mse", "mae"],
              "min_samples_split": [10, 20, 40],
              "max_depth": [2, 6, 8],
              "min_samples_leaf": [20, 40, 100],
              "max_leaf_nodes": [5, 20, 100],
              }

et = ExtraTreesRegressor() 
clf = GridSearchCV(et, param_grid=parameters)
clf.fit(X_train, y_train)

## PREDICTIONS : Extra Tree
results_rmse_et = []
for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(df_n['X'].values.reshape(-1,1), df_n['y'].values.reshape(-1,1), test_size=0.2, shuffle=False)#, shuffle=False, stratify=None, random_state=None)
  # Treinamento
  et = ExtraTreesRegressor(criterion = clf.best_params_['criterion'], max_depth = clf.best_params_['max_depth'], max_leaf_nodes = clf.best_params_['max_leaf_nodes'], min_samples_leaf = clf.best_params_['min_samples_leaf'], min_samples_split = clf.best_params_['min_samples_split']) 
  et.fit(X_train, y_train) 
  # Teste
  y_pred_et = et.predict(X_test)

  # Metrics
  mse_et = mean_squared_error(y_test, y_pred_et)
  rmse_et = np.sqrt(mse_et)
  results_rmse_et.append(rmse_et)
df_results_rmse['et'] = results_rmse_et

##Imprimindo o dataFrame dt
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_et_desnormalized = scaler.inverse_transform([y_pred_dt])
# Visualising the results
df_et = pd.DataFrame()
df_et['real'] = y_test_desnormalized[:,0].tolist()
df_et['predicted et'] = y_pred_et_desnormalized.tolist()[0]
df_et

# Visualising the results dt
plt.plot(df_et.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_et.index, y_pred_et_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

df_results_rmse

df_results_rmse['lr'].mean()

"""# ENSEMBLE"""

## RMSE Metrics
print('LR=', df_results_rmse['lr'].mean())
print( 'SVR=',df_results_rmse['svr'].mean())
print('RF=',df_results_rmse['rf'].mean())
print( 'Knn=',df_results_rmse['knn'].mean())
print( 'Gbr=',df_results_rmse['gbr'].mean())
print( 'abr=',df_results_rmse['abr'].mean())
print( 'rcv=',df_results_rmse['rcv'].mean())
print( 'dt=',df_results_rmse['dt'].mean())
print( 'et=',df_results_rmse['et'].mean())

## ENSEMBLE (BEST 3 - Median)
y_ensemble = []
for i in range(len(y_pred_RF)):
  y_ensemble.append((y_pred_rcv[i] + y_pred_LR[i] + y_pred_SVR[i]) / 3)

  # Metrics
mse_E = mean_squared_error(y_test, y_ensemble)
rmse_E = np.sqrt(mse_E)
rmse_E

print('E=',rmse_E)

y_ensemble_desnormalized = scaler.inverse_transform(y_ensemble)

fig, ax = pyplot.subplots()
pyplot.plot(range(len(y_pred_LR)), y_pred_LR, c='blue', label='Linear Regression')
pyplot.plot(range(len(y_pred_SVR)), y_pred_SVR, c='orange', label='SVR')
#pyplot.plot(range(len(y_pred_RF)), y_pred_RF, c='brown', label='Random Forest')
#pyplot.plot(range(len(y_pred_knn)), y_pred_knn, c='blue', label='KNeighbors')
#pyplot.plot(range(len(y_pred_gbr)), y_pred_gbr, c='gray', label='Gradient Boosting')
#pyplot.plot(range(len(y_pred_abr)), y_pred_abr, c='blue', label='AdaBoost')
pyplot.plot(range(len(y_pred_rcv)), y_pred_rcv, c='pink', label='Ridge CV')
#pyplot.plot(range(len(y_pred_dt)), y_pred_dt, c='red', label='Decision Tree')
#pyplot.plot(range(len(y_pred_et)), y_pred_et, c='orange', label='Extra Tree')
pyplot.plot(range(len(y_ensemble)), y_ensemble, c='green', label='Ensemble')
pyplot.plot(range(len(y_test)), y_test, c='red', label='Real')
fig.set_size_inches(30, 10.5, forward=True)
leg = pyplot.legend();
pyplot.show()

fig, ax = pyplot.subplots()
pyplot.plot(range(len(y_pred_LR_desnormalized)), y_pred_LR_desnormalized, c='blue', label='Linear Regression')
pyplot.plot(range(len(y_pred_svr_desnormalized[0])), y_pred_svr_desnormalized[0], c='orange', label='SVR')
#pyplot.plot(range(len(y_pred_RF)), y_pred_RF, c='brown', label='Random Forest')
#pyplot.plot(range(len(y_pred_knn)), y_pred_knn, c='blue', label='KNeighbors')
#pyplot.plot(range(len(y_pred_gbr)), y_pred_gbr, c='gray', label='Gradient Boosting')
#pyplot.plot(range(len(y_pred_abr)), y_pred_abr, c='blue', label='AdaBoost')
pyplot.plot(range(len(y_pred_rcv_desnormalized)), y_pred_rcv_desnormalized, c='pink', label='Ridge CV')
#pyplot.plot(range(len(y_pred_dt)), y_pred_dt, c='red', label='Decision Tree')
#pyplot.plot(range(len(y_pred_et)), y_pred_et, c='orange', label='Extra Tree')
pyplot.plot(range(len(y_ensemble_desnormalized)), y_ensemble_desnormalized, c='green', label='Ensemble')
pyplot.plot(range(len(y_test_desnormalized)), y_test_desnormalized, c='red', label='Real')
fig.set_size_inches(30, 10.5, forward=True)
leg = pyplot.legend();
pyplot.show()

## GRID SEARCH : MLPRegressor
df_mlp = pd.DataFrame({'LR' : y_pred_LR[:,0].tolist(), 
                     'SVR': y_pred_SVR.tolist(),
                     'RCV': y_pred_rcv[:,0].tolist(),
                     'y'  : y_test[:,0].tolist()})

y_mlp = df_mlp['y']
X_mlp = df_mlp.drop(columns=['y'])

X_trainMLP, X_testMLP, y_trainMLP, y_testMLP = train_test_split(X_mlp.values, y_mlp.values.reshape(-1,1), test_size=0.3,  shuffle=False, stratify=None)
  
#GridSearch (seleção de parâmetros)
parameters = {"hidden_layer_sizes": [(1,),(50,)], "activation": ["identity", "logistic", "tanh", "relu"], "solver": ["lbfgs", "sgd", "adam"], "alpha": [0.00005,0.0005]}

mlp = MLPRegressor()
clf = GridSearchCV(mlp, param_grid=parameters)
clf.fit(X_trainMLP, y_trainMLP)

## ENSEMBLE (LR + SVR + RCV | MLP)

from sklearn.neural_network import MLPRegressor
type(y_pred_LR[:,0])
type(y_test[:,0])
df_e = pd.DataFrame({'LR' : y_pred_LR[:,0].tolist(), 
                     'SVR': y_pred_SVR[:].tolist(),
                     'RCV': y_pred_rcv[:,0].tolist(),
                     'y'  : y_test[:,0].tolist()})
y_e = df_e['y']
X_e = df_e.drop(columns=['y'])

results_rmse_ensemble = []

for i in range(0, 30):
  X_train, X_test, y_train, y_test = train_test_split(X_e.values, y_e.values.reshape(-1,1), test_size=0.3,  shuffle=False, stratify=None)#, random_state=0
  mlp = MLPRegressor(max_iter=150, activation = clf.best_params_['activation'], alpha = clf.best_params_['alpha'], hidden_layer_sizes = clf.best_params_['hidden_layer_sizes'], solver = clf.best_params_['solver'])#random_state=1
  mlp = mlp.fit(X_train, y_train)
  y_pred_En = mlp.predict(X_test)
    # Metrics
  mse_E = mean_squared_error(y_test, y_pred_En)
  rmse_E = np.sqrt(mse_E)
  results_rmse_ensemble.append(rmse_E)
df_results_rmse['ensemble'] = results_rmse_ensemble

df_results_rmse

##Imprimindo o dataFrame svr
y_test_desnormalized = scaler.inverse_transform(y_test)
y_pred_en_desnormalized = scaler.inverse_transform([y_pred_En])
# Visualising the results
df_en = pd.DataFrame()
df_en['real'] = y_test_desnormalized[:,0].tolist()
df_en['predicted ensenble'] = y_pred_en_desnormalized.tolist()[0]
df_en

# Visualising the results SVR
plt.plot(df_en.index, y_test_desnormalized, color = 'red', label = 'Real value')
plt.plot(df_en.index, y_pred_en_desnormalized[0], color = 'blue', label = 'Predicted value')
plt.title('Previsão diaria de vendas')
plt.xlabel('Time')
plt.ylabel('valor')
plt.legend()
plt.show()

## RMSE Metrics
print('LR=', df_results_rmse['lr'].mean())
print( 'SVR=',df_results_rmse['svr'].mean())
print('RF=',df_results_rmse['rf'].mean())
print( 'KNN=',df_results_rmse['knn'].mean())
print( 'GBR=',df_results_rmse['gbr'].mean())
print( 'ABR=',df_results_rmse['abr'].mean())
print( 'RCV=',df_results_rmse['rcv'].mean())
print( 'DT=',df_results_rmse['dt'].mean())
print( 'ET=',df_results_rmse['et'].mean())
print( 'Ensemble=',df_results_rmse['ensemble'].mean())

df_results_rmse.describe()

# teste de hipótese: SVR
import pandas as pd
from scipy import stats
antes  = df_results_rmse['svr']
depois = df_results_rmse['ensemble']

ttest, pval = stats.ttest_rel(antes, depois)
print('stat=%.4f, pval=%.4f, Erro_antes=%.4f, Erro_depois=%.4f' % (ttest, pval, antes.mean(), depois.mean()))

# H0 :- means difference between two sample is 0
# H1 :- mean difference between two sample is not 0

if pval < 0.05:
    if (antes.mean() > depois.mean()):
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve melhoria")
    else:
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve piora")
else:
    print("Aceita H0. Nada pode ser concluído. Não Há evidência estatística com significância de 5% que houve alteração")

import pandas as pd
from scipy import stats
antes  = df_results_rmse['lr']
depois = df_results_rmse['ensemble']

ttest, pval = stats.ttest_rel(antes, depois)
print('stat=%.4f, pval=%.4f, Erro_antes=%.4f, Erro_depois=%.4f' % (ttest, pval, antes.mean(), depois.mean()))

# H0 :- means difference between two sample is 0
# H1 :- mean difference between two sample is not 0

if pval < 0.05:
    if (antes.mean() > depois.mean()):
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve melhoria")
    else:
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve piora")
else:
    print("Aceita H0. Nada pode ser concluído. Não Há evidência estatística com significância de 5% que houve alteração")

import pandas as pd
from scipy import stats
antes  = df_results_rmse['rcv']
depois = df_results_rmse['ensemble']

ttest, pval = stats.ttest_rel(antes, depois)
print('stat=%.4f, pval=%.4f, Erro_antes=%.4f, Erro_depois=%.4f' % (ttest, pval, antes.mean(), depois.mean()))

# H0 :- means difference between two sample is 0
# H1 :- mean difference between two sample is not 0

if pval < 0.05:
    if (antes.mean() > depois.mean()):
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve melhoria")
    else:
      print("Rejeita H0. H1 é aceitável. Há evidência estatística com significância de 5% que houve piora")
else:
    print("Aceita H0. Nada pode ser concluído. Não Há evidência estatística com significância de 5% que houve alteração")