# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19UxRqzTXFjuqPRiIGr6aLKdcAm_DGPin

## **ATIVIDADE 6 - Classificação** 

**Grupo**: Tâmara Dallegrave, Marcus Silva, Daniel Neto, Paulo Brasil.

## **CONFIGURAÇÕES**
"""

# Commented out IPython magic to ensure Python compatibility.
#BIBLIOTECAS

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
import pandas as pd
import seaborn as sns
import statistics
import datetime as dt
import calendar

# %matplotlib inline

!pip install PyDrive
!pip install PyCaret

from sklearn.impute import SimpleImputer
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict
from pycaret.classification import *

# IMPORTAÇÃO DB


auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':"138bRT5WPXfCdZj2fS_sMars4NlaQr75y"})
downloaded.GetContentFile('Sales2.csv')

# CARREGANDO DB
df = pd.read_csv('Sales2.csv', sep=';', na_values='?')

df.head()

"""## **ANÁLISE DA DB**"""

#PRÉ-PROCESSAMENTO DA BASE DE DADOS

#sns.heatmap(df.isna())
df['Timestamp']= pd.to_datetime(df['Timestamp'],format='%Y-%m-%d')
df['year']= df['Timestamp'].dt.year
df['month']= df['Timestamp'].dt.month
df['day']= df['Timestamp'].dt.day
df['hour'] = df['Timestamp'].dt.hour
df['Weekday'] = df['Timestamp'].dt.day_name()
df['PaymentMethod'] = np.where(df['PaymentMethod'] > 1, 'Credit Card', 'Cash')
#df = df.drop(columns='Timestamp')
df.head()

#Gráfico de quantidade de vendas por dia da semana em cada ano
sales_of_weekDay_per_years = df.groupby(['Weekday'])['year'].value_counts()
sales_of_weekDay_per_years = sales_of_weekDay_per_years.unstack()
sales_of_weekDay_per_years.plot(kind='line',figsize=(7, 7))

"""## **PRÉ-PROCESSAMENTO**

No Pré-processamento foi analisado os dados da nossa base de dados para seleção de um cenário problema para classificação.

**Objetivo**: Classificação de dias entre bom e ruim dependendo da quantidade de vendas realizadas naquele dia. Para definição de bom foi utilizado o valor da média de vendas global, se o valor for igual ou maior a média este dia é considerado bom. Caso contrário, o dia é classificado como ruim.
"""

#CÁLCULO DE DIAS DA SEMANA (na base de dados)

'''
Verificação de quantos dias da semana existem na base de dados
'''

start = df['Timestamp'].min().strftime('%Y-%m-%d')
end = df['Timestamp'].max().strftime('%Y-%m-%d')

weekday = ['Sat', 'Fri', 'Tue', 'Mon', 'Thu', 'Wed', 'Sun']


qnt_weekday = []
for elem in weekday:
  qtd = np.busday_count(start, end, weekmask=elem)
  qnt_weekday.append((elem,qtd))

qnt_weekday

# Verificação das médias de venda por dia da semana

'''
Foi verificados que por dia da semana todas as médias possuiam um valor muito próximo como pode ser observado abaixo:
'''

new_df = df.groupby(['Weekday']).mean()
new_df

#CÁLCULO DE VENDAS (por dia da semana)
'''
Este era o algoritmo para calculo da média por dia da semana, este algoritmo foi descartado porque o classificador dividiu os dados de forma desbalanceada.

weekday_rec = df['Weekday'].value_counts()
weekday_mean = []
weekday_values = []
for elem in weekday_rec:
  weekday_values.append(elem)

i = 0
for elem, value in qnt_weekday:
    weekday_mean.append((elem,round(weekday_values[i]/value)))
    i += 1
weekday_mean
'''

#Verificar a quantidade de dias na base de dados
'''
Para média global foi contabilizado a quantidade total de dias da base de dados
'''

df_without_nulls = df.dropna()
totaldays = df_without_nulls['Weekday'].value_counts()
len(totaldays)

#QUANTIDADE DE VENDAS (por dia)

df_class = df.resample('D', on='Timestamp')['Price'].sum().reset_index(name='Amount')
df_class2 = df[['Timestamp', 'Price']].groupby(pd.Grouper(key='Timestamp', freq='1D')).sum()['Price']

#df.resample('D', on='Timestamp')['Price'].sum().value_counts().rename_axis('date').reset_index(name='count'))
df_class2

#CRIAÇÃO DATAFRAME (com quantidade de vendas e valor total vendido por dia)

result = df_class.merge(df_class2, how='inner', on='Timestamp')
result = result.rename(columns={'Price': 'Qty'})

result['Timestamp'] = pd.to_datetime(result['Timestamp'],format='%Y-%m-%d')
result['Weekday'] = result['Timestamp'].dt.day_name()
result['Avg'] = result['Amount']/result['Qty']
#len(result) # 1157

# Remoção de dados nulos
result_without_NaN = result.dropna()

# MÉDIA DE VENDAS GLOBAL
qty_days = len(result_without_NaN)
totalAmount = result_without_NaN['Amount'].sum()
totalAmount
day_avg_sold = round(totalAmount/qty_days)
day_avg_sold

#CRIAÇÃO DA COLUNA DE SAÍDA (DO CLASSIFICADOR)
result = result_without_NaN.drop(columns='Timestamp')
result['Analysis'] = np.where(result['Amount'] >= day_avg_sold, 'Good', 'Bad')

result.loc[result['Analysis'] == 'Bad']
result

#CATEGORIZAÇÃO: Transformando campos não-numéricos em numéricos
le = preprocessing.LabelEncoder()
result['Weekday'] = le.fit_transform(result['Weekday'])
result['Analysis'] = le.fit_transform(result['Analysis'])

result.head()

# Correlação das features com o target(Analysis)
sns.heatmap(result.corr(), annot=True, cmap='YlGnBu')

"""## **CLASSIFICAÇÃO**"""

### Selecionar variáveis de entrada e saída

x = result.drop(columns='Analysis') # x são das entradas
x = x.drop(columns='Avg')
y = result['Analysis'] # y é a saída esperada


#NORMALIZAÇÃO
x = (x-x.min())/(x.max()-x.min())

### Definir dados de treino e teste
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

"""**Inferência do tipo de dados:**  Determinação dos tipos de dados correctos para todas as características. A função de configuração executa inferências essenciais sobre os dados e executa várias tarefas a jusante, tais como ignorar as colunas ID e Data, codificação categórica, imputação de valores em falta com base no tipo de dados inferidos pelo algoritmo interno do PyCaret. Esta função lista de todas as características e os seus tipos de dados inferidos."""

#SETUP 
'''

'''

exp_clf101 = setup(data = result, target = 'Analysis', session_id=123)

"""## **Naive Bayes**

O algoritmo consiste em encontrar uma probabilidade a posteriori (possuir a doença, dado que recebeu um resultado positivo), multiplicando a probabilidade a priori (possuir a doença) pela probabilidade de “receber um resultado positivo, dado que tem a doença”. Porém, ele desconsidera completamente a correlação entre as variáveis (features). Ou seja, se determinada fruta é considerada uma “Maçã” se ela for “Vermelha”, “Redonda” e possui “aproximadamente 10cm de diâmetro”, o algoritmo não vai levar em consideração a correlação entre esses fatores, tratando cada um de forma independente.

Por possuir uma velocidade relativamente alta e precisar apenas de poucos dados para realizar a classificação, pode ser utilizado para previsões em tempo real. Este algoritmo é muito utilizado para filtragem de SPAM, Análise de Sentimento nas redes sociais (identificar se o usuário está feliz ou triste ao publicar determinado texto).


"""

X_train

## Naive Bayes
nb = GaussianNB()

## treinando classificador
nb.fit(X_train, y_train)

## testanto classificador
res_nb = nb.predict(X_test)

print(classification_report(y_test, res_nb))

"""### **KNeighborsClassifier**

É um algoritmo não paramétrico, aonde a estrutura do modelo será determinada pelo dataset utilizado. Este algoritmo também é conhecido como de aprendizado lento ou seja, não necessitam de dados de treinamento para se gerar o modelo, o que diminui em partes o processo inicial, mas em contrapartida gerará uma necessidade de analise posterior mais apurada. Todos os dados obtidos no dataset serão utilizados na fase de teste, resultando em um treinamento muito rápido e em um teste e validação lentos, momento o qual necessitamos estar bem atentos aos resultados gerados. 
É um classificador onde o aprendizado é baseado “no quão similar” é um dado (um vetor) do outro. O treinamento é formado por vetores de n dimensões.

As etapas de um algoritmo KNN são:

```
1 — Recebe um dado não classificado;
2 — Mede a distância (Euclidiana, Manhattan, Minkowski ou Ponderada) do novo; dado com todos os outros dados que já estão classificados;
3 — Obtém as X(no caso essa variável X é o parâmetro K) menores distâncias;
4 — Verifica a classe de cada da um dos dados que tiveram a menor distância e conta a quantidade de cada classe que aparece;
5 — Toma como resultado a classe que mais apareceu dentre os dados que tiveram as menores distâncias;
6 — Classifica o novo dado com a classe tomada como resultado da classificação

```


"""

from sklearn.pipeline import Pipeline
model_name = "KNeighborsClassifier"
knnClassifier = KNeighborsClassifier(n_neighbors = 5, metric='minkowski', p=2)
knnClassifier.fit(X_train, y_train)
y_pred_knn = knnClassifier.predict(X_test)
print(classification_report(y_test, y_pred_knn))

"""### **Árvore de Decisão**
Uma árvore de decisão é formada por um conjunto de regras de classificação. Cada caminho da raiz até uma folha representa uma destas regras. A árvore de decisão deve ser definida de forma que, para cada observação da base de dados, haja um e apenas um caminho da raiz até a folha
"""

# Árvore de Decisão

dtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)
dtc = dtc.fit(X_train, y_train)

res_dtc = dtc.predict(X_test)

print(classification_report(y_test, res_dtc))

"""### **Regressão Logística**
É recomendada para situações em que a
variável dependente é de natureza dicotômica ou
binária. Quanto às independentes, tanto podem ser
categóricas ou não.

Busca estimar a probabilidade da variável
dependente assumir um determinado valor em
função dos conhecidos de outras variáveis.

Os resultados da análise ficam contidos no
intervalo de zero a um.
"""

# Regressão Logística
from sklearn.linear_model import LogisticRegression 
model_name = "Logistic Regression Classifier"
logisticRegressionClassifier = LogisticRegression(random_state=0,max_iter=1000)
logisticRegressionClassifier.fit(X_train,y_train)
y_pred_lrc = logisticRegressionClassifier.predict(X_test)
print(classification_report(y_test, y_pred_lrc))

"""### **Comparação de Modelos**"""

compare_models()